# -*- coding: utf-8 -*-
"""LINEAR REGRESSION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qd7eEoPnkoCluSxH6ATi19kyVSwnWkIU
"""

import numpy as np
import matplotlib.pyplot as plt

samples = 30

hours = np.random.randint(1, 11, samples)
rank = 10 - 0.9 * hours + np.random.normal(0, 1, samples)

X = hours.reshape(-1, 1)
y = rank

X_bias = np.c_[np.ones((X.shape[0], 1)), X]

def weight(X, y):
    weights = np.linalg.inv(X.transpose() @ X) @ X.transpose() @ y
    return weights

def prediction(X, weights):
    return X @ weights

def GD(X, y, learningrate=0.02, iterations=1000, tolerance=1e-8):
    n = len(X)
    weights = np.zeros(X.shape[1])
    errors = []
    for iteration in range(iterations):
        predictions = X @ weights
        weights = weights - learningrate * -2/n * X.transpose() @ (y - predictions)
        MSE = np.mean((y - predictions) ** 2)
        errors.append(MSE)
        if iteration > 0 and abs(errors[-1] - errors[-2]) <= tolerance:
            break
    return weights, errors

closed_form_W = weight(X_bias, y)
print("WEIGHTS FOR THE CLOSED FORM SOLUTION ARE (w0, w1): ", closed_form_W)
predictions_closed_form = prediction(X_bias, closed_form_W)

weights_gd, training_errors = GD(X_bias, y, learningrate=0.01, iterations=1000)
print("WEIGHTS FOR THE GRADIENT DESCENT ALGORITHM ARE (w0, w1): ", weights_gd)
predictions_gd = prediction(X_bias, weights_gd)

plt.figure(figsize=(8,5))
plt.plot(training_errors, label="TRAINING ERROR FOR GRADIENT DESCENT", color='GREEN', linewidth=3)
plt.title("GRADIENT DESCENT - TRAINING ERROR VS NUMBER OF EPOCS")
plt.ylabel("TRAINING ERROR")
plt.xlabel("NUMBER OF EPOCHS")
plt.grid(True)
plt.legend()
plt.show()

plt.figure(figsize=(8,5))
plt.scatter(X, y, color='BLUE', label="SAMPLE DATA", s=60)
plt.title("LINEAR REGRESSION MODEL")
plt.plot(X, predictions_closed_form, label="CLOSED FORM SOLUTION", color='RED', linewidth=2.5)
plt.plot(X, predictions_gd, label="GRADIENT DESCENT SOLUTION", color='DARKGREEN', linewidth=2.5)
plt.xlabel("NUMBER OF HOURS STUDIED")
plt.ylabel("RANK")
plt.grid(True)
plt.legend()
plt.show()

print("\nWEIGHTS FOR THE CLOSED FORM SOLUTION:", closed_form_W)
print("GRADIENT DESCENT WEIGHTS: ", weights_gd)
print(f"DIFFERENCE: {np.abs(closed_form_W - weights_gd)}")

def polynomial(X, degree):
    return np.hstack([X**i for i in range(degree + 1)])

degree = 4
X_4 = polynomial(X, degree)
weights_4 = weight(X_4, y)
print(f"\nPOLYNOMIAL COEFFICIENTS FOR 4TH DEGREE: {weights_4}")

degrees = [4, 6, 8, 10, 12, 14, 16]
training_errors = {}
plt.figure(figsize=(8,5))

for degree in degrees:
    X_poly = polynomial(X, degree)
    weights_poly = weight(X_poly, y)
    weights_gd_poly, errors = GD(X_poly, y)
    training_errors[degree] = errors
    print(f"{degree}TH DEGREE POLYNOMIAL COEFFICIENTS: {weights_poly}")
    plt.plot(errors, label=f"DEGREE {degree}")

plt.title("TRAINING ERROR VS NUMBER OF ITERATIONS FOR HIGHER DEGREE POLYNOMIALS")
plt.xlabel("NUMBER OF ITERATIONS")
plt.ylabel("TRAINING ERROR")
plt.legend()
plt.grid(True)
plt.show()